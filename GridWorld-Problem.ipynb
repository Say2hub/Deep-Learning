{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###*Sayantan Mukherjee 60009220131*"
      ],
      "metadata": {
        "id": "qU_0skko6PLF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZ9Px5uUgbyE",
        "outputId": "1074f062-e538-4d58-99ba-7735d2a49335"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iterative Policy Evaluation:\n",
            "[[0.   0.   0.  ]\n",
            " [0.   1.71 1.9 ]\n",
            " [0.   0.   1.  ]]\n",
            "\n",
            "Policy Iteration:\n",
            "[[1 1 1]\n",
            " [1 1 1]\n",
            " [3 3 1]]\n",
            "\n",
            "Value Iteration:\n",
            "[[1 1 1]\n",
            " [1 1 1]\n",
            " [3 3 1]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class GridWorld:\n",
        "    def __init__(self):\n",
        "        self.grid_size = 3\n",
        "        self.num_states = self.grid_size * self.grid_size\n",
        "        self.num_actions = 4\n",
        "        self.actions = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
        "        self.gamma = 0.9\n",
        "        self.grid_rewards = np.zeros((self.grid_size, self.grid_size))\n",
        "        self.grid_rewards[self.grid_size - 1, self.grid_size - 1] = 1  #goal reward\n",
        "        self.transition_prob = self._build_transition_prob()\n",
        "\n",
        "    def _build_transition_prob(self):\n",
        "        transition_prob = np.zeros((self.num_states, self.num_actions, self.num_states))\n",
        "        for s in range(self.num_states):\n",
        "            for a in range(self.num_actions):\n",
        "                next_states = self._get_next_states(s, a)\n",
        "                for next_state, prob in next_states.items():\n",
        "                    transition_prob[s, a, next_state] = prob\n",
        "        return transition_prob\n",
        "\n",
        "    def _get_next_states(self, state, action):\n",
        "        i, j = state // self.grid_size, state % self.grid_size\n",
        "        next_states = {}\n",
        "        if action == 0:\n",
        "            next_i = max(i - 1, 0)\n",
        "            next_states[next_i * self.grid_size + j] = 1\n",
        "        elif action == 1:\n",
        "            next_i = min(i + 1, self.grid_size - 1)\n",
        "            next_states[next_i * self.grid_size + j] = 1\n",
        "        elif action == 2:\n",
        "            next_j = max(j - 1, 0)\n",
        "            next_states[i * self.grid_size + next_j] = 1\n",
        "        elif action == 3:\n",
        "            next_j = min(j + 1, self.grid_size - 1)\n",
        "            next_states[i * self.grid_size + next_j] = 1\n",
        "        return next_states\n",
        "\n",
        "    def iterative_policy_eval(self, policy, theta=0.0001, max_iterations=1000):\n",
        "        V = np.zeros(self.num_states)\n",
        "        for _ in range(max_iterations):\n",
        "            delta = 0\n",
        "            for s in range(self.num_states):\n",
        "                v = V[s]\n",
        "                if s == (self.num_states - 1):\n",
        "                    V[s] = self.grid_rewards[s // self.grid_size, s % self.grid_size]\n",
        "                else:\n",
        "                    a = policy[s]\n",
        "                    V[s] = sum([self.transition_prob[s, a, s1] *\n",
        "                                (self.grid_rewards[s1 // self.grid_size, s1 % self.grid_size] +\n",
        "                                 self.gamma * V[s1]) for s1 in range(self.num_states)])\n",
        "                delta = max(delta, abs(v - V[s]))\n",
        "            if delta < theta:\n",
        "                break\n",
        "        return V\n",
        "\n",
        "    def policy_itr(self, max_iterations=100):\n",
        "        policy = np.random.randint(0, self.num_actions, self.num_states)\n",
        "        for _ in range(max_iterations):\n",
        "            V = self.iterative_policy_eval(policy)\n",
        "            policy_stable = True\n",
        "            for s in range(self.num_states):\n",
        "                old_action = policy[s]\n",
        "                policy[s] = np.argmax([sum([self.transition_prob[s, a, s1] *\n",
        "                                             (self.grid_rewards[s1 // self.grid_size, s1 % self.grid_size] +\n",
        "                                              self.gamma * V[s1]) for s1 in range(self.num_states)]) for a in range(self.num_actions)])\n",
        "                if old_action != policy[s]:\n",
        "                    policy_stable = False\n",
        "            if policy_stable:\n",
        "                break\n",
        "        return policy\n",
        "\n",
        "    def value_itr(self, theta=0.0001, max_iterations=1000):\n",
        "        V = np.zeros(self.num_states)\n",
        "        for _ in range(max_iterations):\n",
        "            delta = 0\n",
        "            for s in range(self.num_states):\n",
        "                v = V[s]\n",
        "                if s == (self.num_states - 1):\n",
        "                    V[s] = self.grid_rewards[s // self.grid_size, s % self.grid_size]\n",
        "                else:\n",
        "                    V[s] = max([sum([self.transition_prob[s, a, s1] *\n",
        "                                     (self.grid_rewards[s1 // self.grid_size, s1 % self.grid_size] +\n",
        "                                      self.gamma * V[s1]) for s1 in range(self.num_states)]) for a in range(self.num_actions)])\n",
        "                delta = max(delta, abs(v - V[s]))\n",
        "            if delta < theta:\n",
        "                break\n",
        "        policy = np.zeros(self.num_states, dtype=int)\n",
        "        for s in range(self.num_states):\n",
        "            policy[s] = np.argmax([sum([self.transition_prob[s, a, s1] *\n",
        "                                         (self.grid_rewards[s1 // self.grid_size, s1 % self.grid_size] +\n",
        "                                          self.gamma * V[s1]) for s1 in range(self.num_states)]) for a in range(self.num_actions)])\n",
        "        return policy\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    grid_world = GridWorld()\n",
        "\n",
        "    policy = np.random.randint(0, grid_world.num_actions, grid_world.num_states)\n",
        "    V = grid_world.iterative_policy_eval(policy)\n",
        "    print(\"Iterative Policy Evaluation:\")\n",
        "    print(V.reshape(grid_world.grid_size, grid_world.grid_size))\n",
        "\n",
        "    policy = grid_world.policy_itr()\n",
        "    print(\"\\nPolicy Iteration:\")\n",
        "    print(policy.reshape(grid_world.grid_size, grid_world.grid_size))\n",
        "\n",
        "    policy = grid_world.value_itr()\n",
        "    print(\"\\nValue Iteration:\")\n",
        "    print(policy.reshape(grid_world.grid_size, grid_world.grid_size))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- **Setup**: A simple 3x3 grid with a goal at (2,2) offering a reward of 1, discount factor (gamma) of 0.9, and four actions (UP, DOWN, LEFT, RIGHT). No obstacles.\n",
        "- **Algorithms**:\n",
        "  - **Iterative Policy Evaluation**: Evaluates a random policy, resulting in a value function reflecting expected rewards (e.g., highest value near the goal at 1.9).\n",
        "  - **Policy Iteration**: Iteratively improves a random policy, converging to an optimal policy (mostly moving RIGHT and DOWN toward the goal).\n",
        "  - **Value Iteration**: Directly computes the optimal value function and derives the policy, yielding identical results to Policy Iteration.\n",
        "- **Results**:\n",
        "  - Value function shows increasing values toward the goal.\n",
        "  - Policies: 0=UP, 1=DOWN, 2=LEFT, 3=RIGHT; optimal policy directs all moves toward (2,2)."
      ],
      "metadata": {
        "id": "pFAOYAYm7rr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class GridWorld:\n",
        "    def __init__(self, grid_size=16):\n",
        "        if (grid_size & (grid_size - 1)) != 0:\n",
        "            raise ValueError(\"grid_size must be a power of 2 (e.g., 2, 4, 8, 16, ...)\")\n",
        "        self.grid_size = grid_size\n",
        "        self.num_states = self.grid_size * self.grid_size\n",
        "        self.num_actions = 4\n",
        "        self.actions = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
        "        self.gamma = 0.9\n",
        "        self.grid_rewards = np.zeros((self.grid_size, self.grid_size))\n",
        "        self.grid_rewards[self.grid_size - 1, self.grid_size - 1] = 1  # Goal state\n",
        "        self.transition_prob = self._build_transition_prob()\n",
        "\n",
        "    def _build_transition_prob(self):\n",
        "        transition_prob = np.zeros((self.num_states, self.num_actions, self.num_states))\n",
        "        for s in range(self.num_states):\n",
        "            for a in range(self.num_actions):\n",
        "                next_states = self._get_next_states(s, a)\n",
        "                for next_state, prob in next_states.items():\n",
        "                    transition_prob[s, a, next_state] = prob\n",
        "        return transition_prob\n",
        "\n",
        "    def _get_next_states(self, state, action):\n",
        "        i, j = state // self.grid_size, state % self.grid_size\n",
        "        next_states = {}\n",
        "        if action == 0:  # UP\n",
        "            next_i = max(i - 1, 0)\n",
        "            next_states[next_i * self.grid_size + j] = 1\n",
        "        elif action == 1:  # DOWN\n",
        "            next_i = min(i + 1, self.grid_size - 1)\n",
        "            next_states[next_i * self.grid_size + j] = 1\n",
        "        elif action == 2:  # LEFT\n",
        "            next_j = max(j - 1, 0)\n",
        "            next_states[i * self.grid_size + next_j] = 1\n",
        "        elif action == 3:  # RIGHT\n",
        "            next_j = min(j + 1, self.grid_size - 1)\n",
        "            next_states[i * self.grid_size + next_j] = 1\n",
        "        return next_states\n",
        "\n",
        "    def iterative_policy_eval(self, policy, theta=1e-4, max_iterations=1000):\n",
        "        V = np.zeros(self.num_states)\n",
        "        iterations = 0\n",
        "        for _ in range(max_iterations):\n",
        "            iterations += 1\n",
        "            delta = 0\n",
        "            for s in range(self.num_states):\n",
        "                if s == self.num_states - 1:  # Skip terminal state\n",
        "                    continue\n",
        "                v = V[s]\n",
        "                a = policy[s]\n",
        "                V[s] = sum(\n",
        "                    self.transition_prob[s, a, s1] *\n",
        "                    (self.grid_rewards[s1 // self.grid_size, s1 % self.grid_size] + self.gamma * V[s1])\n",
        "                    for s1 in range(self.num_states)\n",
        "                )\n",
        "                delta = max(delta, abs(v - V[s]))\n",
        "            if delta < theta:\n",
        "                break\n",
        "        return V, iterations\n",
        "\n",
        "    def policy_itr(self, theta=1e-4, max_iterations=1000):\n",
        "        policy = np.random.randint(0, self.num_actions, self.num_states)\n",
        "        policy_stable = False\n",
        "        total_eval_iterations = 0  # Track total inner iterations\n",
        "        outer_iterations = 0\n",
        "\n",
        "        while not policy_stable and outer_iterations < max_iterations:\n",
        "            outer_iterations += 1\n",
        "            V, eval_iterations = self.iterative_policy_eval(policy, theta, max_iterations)\n",
        "            total_eval_iterations += eval_iterations\n",
        "            policy_stable = True\n",
        "            for s in range(self.num_states):\n",
        "                if s == self.num_states - 1:  # Skip terminal state\n",
        "                    continue\n",
        "                old_action = policy[s]\n",
        "                action_values = np.zeros(self.num_actions)\n",
        "                for a in range(self.num_actions):\n",
        "                    action_values[a] = sum(\n",
        "                        self.transition_prob[s, a, s1] *\n",
        "                        (self.grid_rewards[s1 // self.grid_size, s1 % self.grid_size] + self.gamma * V[s1])\n",
        "                        for s1 in range(self.num_states)\n",
        "                    )\n",
        "                policy[s] = np.argmax(action_values)\n",
        "                if old_action != policy[s]:\n",
        "                    policy_stable = False\n",
        "        return policy, V, total_eval_iterations\n",
        "\n",
        "    def value_itr(self, theta=1e-4, max_iterations=1000):\n",
        "        V = np.zeros(self.num_states)\n",
        "        iterations = 0\n",
        "        while iterations < max_iterations:\n",
        "            iterations += 1\n",
        "            delta = 0\n",
        "            for s in range(self.num_states):\n",
        "                if s == self.num_states - 1:  # Skip terminal state\n",
        "                    continue\n",
        "                v = V[s]\n",
        "                V[s] = max(\n",
        "                    sum(\n",
        "                        self.transition_prob[s, a, s1] *\n",
        "                        (self.grid_rewards[s1 // self.grid_size, s1 % self.grid_size] + self.gamma * V[s1])\n",
        "                        for s1 in range(self.num_states)\n",
        "                    )\n",
        "                    for a in range(self.num_actions)\n",
        "                )\n",
        "                delta = max(delta, abs(v - V[s]))\n",
        "            if delta < theta:\n",
        "                break\n",
        "        policy = np.zeros(self.num_states, dtype=int)\n",
        "        for s in range(self.num_states):\n",
        "            action_values = np.zeros(self.num_actions)\n",
        "            for a in range(self.num_actions):\n",
        "                action_values[a] = sum(\n",
        "                    self.transition_prob[s, a, s1] *\n",
        "                    (self.grid_rewards[s1 // self.grid_size, s1 % self.grid_size] + self.gamma * V[s1])\n",
        "                    for s1 in range(self.num_states)\n",
        "                )\n",
        "            policy[s] = np.argmax(action_values)\n",
        "        return policy, V, iterations\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    grid_size = 16\n",
        "    grid_world = GridWorld(grid_size=grid_size)\n",
        "\n",
        "    # ---- Iterative Policy Evaluation ----\n",
        "    random_policy = np.random.randint(0, grid_world.num_actions, grid_world.num_states)\n",
        "    V_iter, eval_iterations = grid_world.iterative_policy_eval(random_policy)\n",
        "    print(\"Iterative Policy Evaluation:\")\n",
        "    print(V_iter.reshape(grid_world.grid_size, grid_world.grid_size))\n",
        "    print(f\"Converged in {eval_iterations} iterations\\n\")\n",
        "\n",
        "    # ---- Policy Iteration ----\n",
        "    policy_pi, V_pi, pi_total_iter = grid_world.policy_itr()\n",
        "    print(\"Policy Iteration:\")\n",
        "    print(\"Policy (0=UP, 1=DOWN, 2=LEFT, 3=RIGHT):\")\n",
        "    print(policy_pi.reshape(grid_world.grid_size, grid_world.grid_size))\n",
        "    print(\"Value Function:\")\n",
        "    print(V_pi.reshape(grid_world.grid_size, grid_world.grid_size))\n",
        "    print(f\"Total iterations (including inner evaluations): {pi_total_iter}\\n\")\n",
        "\n",
        "    # ---- Value Iteration ----\n",
        "    policy_vi, V_vi, vi_iterations = grid_world.value_itr()\n",
        "    print(\"Value Iteration:\")\n",
        "    print(\"Policy (0=UP, 1=DOWN, 2=LEFT, 3=RIGHT):\")\n",
        "    print(policy_vi.reshape(grid_world.grid_size, grid_world.grid_size))\n",
        "    print(\"Value Function:\")\n",
        "    print(V_vi.reshape(grid_world.grid_size, grid_world.grid_size))\n",
        "    print(f\"Total iterations: {vi_iterations}\\n\")\n",
        "\n",
        "    # ---- Fair Comparison ----\n",
        "    if pi_total_iter < vi_iterations:\n",
        "        print(\"Policy Iteration converged faster in total iterations.\")\n",
        "    elif pi_total_iter > vi_iterations:\n",
        "        print(\"Value Iteration converged faster in total iterations.\")\n",
        "    else:\n",
        "        print(\"Both methods converged in the same number of iterations.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcVBJQ-0mYta",
        "outputId": "c42eed46-a562-4265-b761-8312e28b1eaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iterative Policy Evaluation:\n",
            "[[0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.9 0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0. ]]\n",
            "Converged in 3 iterations\n",
            "\n",
            "Policy Iteration:\n",
            "Policy (0=UP, 1=DOWN, 2=LEFT, 3=RIGHT):\n",
            "[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            " [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2]]\n",
            "Value Function:\n",
            "[[0.04710129 0.05233476 0.05814974 0.06461082 0.0717898  0.07976644\n",
            "  0.08862938 0.09847709 0.10941899 0.12157665 0.13508517 0.15009464\n",
            "  0.16677182 0.18530202 0.20589113 0.22876792]\n",
            " [0.05233476 0.05814974 0.06461082 0.0717898  0.07976644 0.08862938\n",
            "  0.09847709 0.10941899 0.12157665 0.13508517 0.15009464 0.16677182\n",
            "  0.18530202 0.20589113 0.22876792 0.25418658]\n",
            " [0.05814974 0.06461082 0.0717898  0.07976644 0.08862938 0.09847709\n",
            "  0.10941899 0.12157665 0.13508517 0.15009464 0.16677182 0.18530202\n",
            "  0.20589113 0.22876792 0.25418658 0.28242954]\n",
            " [0.06461082 0.0717898  0.07976644 0.08862938 0.09847709 0.10941899\n",
            "  0.12157665 0.13508517 0.15009464 0.16677182 0.18530202 0.20589113\n",
            "  0.22876792 0.25418658 0.28242954 0.3138106 ]\n",
            " [0.0717898  0.07976644 0.08862938 0.09847709 0.10941899 0.12157665\n",
            "  0.13508517 0.15009464 0.16677182 0.18530202 0.20589113 0.22876792\n",
            "  0.25418658 0.28242954 0.3138106  0.34867844]\n",
            " [0.07976644 0.08862938 0.09847709 0.10941899 0.12157665 0.13508517\n",
            "  0.15009464 0.16677182 0.18530202 0.20589113 0.22876792 0.25418658\n",
            "  0.28242954 0.3138106  0.34867844 0.38742049]\n",
            " [0.08862938 0.09847709 0.10941899 0.12157665 0.13508517 0.15009464\n",
            "  0.16677182 0.18530202 0.20589113 0.22876792 0.25418658 0.28242954\n",
            "  0.3138106  0.34867844 0.38742049 0.43046721]\n",
            " [0.09847709 0.10941899 0.12157665 0.13508517 0.15009464 0.16677182\n",
            "  0.18530202 0.20589113 0.22876792 0.25418658 0.28242954 0.3138106\n",
            "  0.34867844 0.38742049 0.43046721 0.4782969 ]\n",
            " [0.10941899 0.12157665 0.13508517 0.15009464 0.16677182 0.18530202\n",
            "  0.20589113 0.22876792 0.25418658 0.28242954 0.3138106  0.34867844\n",
            "  0.38742049 0.43046721 0.4782969  0.531441  ]\n",
            " [0.12157665 0.13508517 0.15009464 0.16677182 0.18530202 0.20589113\n",
            "  0.22876792 0.25418658 0.28242954 0.3138106  0.34867844 0.38742049\n",
            "  0.43046721 0.4782969  0.531441   0.59049   ]\n",
            " [0.13508517 0.15009464 0.16677182 0.18530202 0.20589113 0.22876792\n",
            "  0.25418658 0.28242954 0.3138106  0.34867844 0.38742049 0.43046721\n",
            "  0.4782969  0.531441   0.59049    0.6561    ]\n",
            " [0.15009464 0.16677182 0.18530202 0.20589113 0.22876792 0.25418658\n",
            "  0.28242954 0.3138106  0.34867844 0.38742049 0.43046721 0.4782969\n",
            "  0.531441   0.59049    0.6561     0.729     ]\n",
            " [0.16677182 0.18530202 0.20589113 0.22876792 0.25418658 0.28242954\n",
            "  0.3138106  0.34867844 0.38742049 0.43046721 0.4782969  0.531441\n",
            "  0.59049    0.6561     0.729      0.81      ]\n",
            " [0.18530202 0.20589113 0.22876792 0.25418658 0.28242954 0.3138106\n",
            "  0.34867844 0.38742049 0.43046721 0.4782969  0.531441   0.59049\n",
            "  0.6561     0.729      0.81       0.9       ]\n",
            " [0.20589113 0.22876792 0.25418658 0.28242954 0.3138106  0.34867844\n",
            "  0.38742049 0.43046721 0.4782969  0.531441   0.59049    0.6561\n",
            "  0.729      0.81       0.9        1.        ]\n",
            " [0.22876792 0.25418658 0.28242954 0.3138106  0.34867844 0.38742049\n",
            "  0.43046721 0.4782969  0.531441   0.59049    0.6561     0.729\n",
            "  0.81       0.9        1.         0.        ]]\n",
            "Total iterations (including inner evaluations): 496\n",
            "\n",
            "Value Iteration:\n",
            "Policy (0=UP, 1=DOWN, 2=LEFT, 3=RIGHT):\n",
            "[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            " [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1]]\n",
            "Value Function:\n",
            "[[0.04710129 0.05233476 0.05814974 0.06461082 0.0717898  0.07976644\n",
            "  0.08862938 0.09847709 0.10941899 0.12157665 0.13508517 0.15009464\n",
            "  0.16677182 0.18530202 0.20589113 0.22876792]\n",
            " [0.05233476 0.05814974 0.06461082 0.0717898  0.07976644 0.08862938\n",
            "  0.09847709 0.10941899 0.12157665 0.13508517 0.15009464 0.16677182\n",
            "  0.18530202 0.20589113 0.22876792 0.25418658]\n",
            " [0.05814974 0.06461082 0.0717898  0.07976644 0.08862938 0.09847709\n",
            "  0.10941899 0.12157665 0.13508517 0.15009464 0.16677182 0.18530202\n",
            "  0.20589113 0.22876792 0.25418658 0.28242954]\n",
            " [0.06461082 0.0717898  0.07976644 0.08862938 0.09847709 0.10941899\n",
            "  0.12157665 0.13508517 0.15009464 0.16677182 0.18530202 0.20589113\n",
            "  0.22876792 0.25418658 0.28242954 0.3138106 ]\n",
            " [0.0717898  0.07976644 0.08862938 0.09847709 0.10941899 0.12157665\n",
            "  0.13508517 0.15009464 0.16677182 0.18530202 0.20589113 0.22876792\n",
            "  0.25418658 0.28242954 0.3138106  0.34867844]\n",
            " [0.07976644 0.08862938 0.09847709 0.10941899 0.12157665 0.13508517\n",
            "  0.15009464 0.16677182 0.18530202 0.20589113 0.22876792 0.25418658\n",
            "  0.28242954 0.3138106  0.34867844 0.38742049]\n",
            " [0.08862938 0.09847709 0.10941899 0.12157665 0.13508517 0.15009464\n",
            "  0.16677182 0.18530202 0.20589113 0.22876792 0.25418658 0.28242954\n",
            "  0.3138106  0.34867844 0.38742049 0.43046721]\n",
            " [0.09847709 0.10941899 0.12157665 0.13508517 0.15009464 0.16677182\n",
            "  0.18530202 0.20589113 0.22876792 0.25418658 0.28242954 0.3138106\n",
            "  0.34867844 0.38742049 0.43046721 0.4782969 ]\n",
            " [0.10941899 0.12157665 0.13508517 0.15009464 0.16677182 0.18530202\n",
            "  0.20589113 0.22876792 0.25418658 0.28242954 0.3138106  0.34867844\n",
            "  0.38742049 0.43046721 0.4782969  0.531441  ]\n",
            " [0.12157665 0.13508517 0.15009464 0.16677182 0.18530202 0.20589113\n",
            "  0.22876792 0.25418658 0.28242954 0.3138106  0.34867844 0.38742049\n",
            "  0.43046721 0.4782969  0.531441   0.59049   ]\n",
            " [0.13508517 0.15009464 0.16677182 0.18530202 0.20589113 0.22876792\n",
            "  0.25418658 0.28242954 0.3138106  0.34867844 0.38742049 0.43046721\n",
            "  0.4782969  0.531441   0.59049    0.6561    ]\n",
            " [0.15009464 0.16677182 0.18530202 0.20589113 0.22876792 0.25418658\n",
            "  0.28242954 0.3138106  0.34867844 0.38742049 0.43046721 0.4782969\n",
            "  0.531441   0.59049    0.6561     0.729     ]\n",
            " [0.16677182 0.18530202 0.20589113 0.22876792 0.25418658 0.28242954\n",
            "  0.3138106  0.34867844 0.38742049 0.43046721 0.4782969  0.531441\n",
            "  0.59049    0.6561     0.729      0.81      ]\n",
            " [0.18530202 0.20589113 0.22876792 0.25418658 0.28242954 0.3138106\n",
            "  0.34867844 0.38742049 0.43046721 0.4782969  0.531441   0.59049\n",
            "  0.6561     0.729      0.81       0.9       ]\n",
            " [0.20589113 0.22876792 0.25418658 0.28242954 0.3138106  0.34867844\n",
            "  0.38742049 0.43046721 0.4782969  0.531441   0.59049    0.6561\n",
            "  0.729      0.81       0.9        1.        ]\n",
            " [0.22876792 0.25418658 0.28242954 0.3138106  0.34867844 0.38742049\n",
            "  0.43046721 0.4782969  0.531441   0.59049    0.6561     0.729\n",
            "  0.81       0.9        1.         0.        ]]\n",
            "Total iterations: 31\n",
            "\n",
            "Value Iteration converged faster in total iterations.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Setup**: Expanded to a 16x16 grid, still with a goal at (15,15) and no obstacles. Grid size must be a power of 2.\n",
        "- **Algorithms**: Same as above, with added iteration tracking for comparison.\n",
        "- **Results**:\n",
        "  - **Iterative Policy Evaluation**: Converges in 3 iterations with a random policy, showing low values due to poor initial policy.\n",
        "  - **Policy Iteration**: Converges with 496 total iterations (including inner evaluations), optimal policy moves DOWN then RIGHT to the goal, value function increases smoothly toward 1.0.\n",
        "  - **Value Iteration**: Converges in 31 iterations, producing the same optimal policy and value function.\n",
        "- **Comparison**: Value Iteration is faster (31 vs. 496 iterations), highlighting its efficiency in larger grids without obstacles."
      ],
      "metadata": {
        "id": "vhYcTTs078wT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class GridWorld:\n",
        "    def __init__(self, grid_size=4):\n",
        "        if (grid_size & (grid_size - 1)) != 0:\n",
        "            raise ValueError(\"grid_size must be a power of 2 (e.g., 2, 4, 8, 16, ...)\")\n",
        "        self.grid_size = grid_size\n",
        "        self.num_states = self.grid_size * self.grid_size\n",
        "        self.num_actions = 4\n",
        "        self.actions = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
        "        self.gamma = 0.9\n",
        "\n",
        "        # Initialize grid rewards and obstacles\n",
        "        self.grid_rewards = np.zeros((self.grid_size, self.grid_size))\n",
        "        self.grid_obstacles = np.zeros((self.grid_size, self.grid_size), dtype=bool)\n",
        "        self._generate_obstacles()\n",
        "\n",
        "        # Goal state (bottom-right)\n",
        "        self.goal = (self.grid_size - 1, self.grid_size - 1)\n",
        "        self.grid_rewards[self.goal] = 1\n",
        "\n",
        "        # Build transition matrices\n",
        "        self.transition_prob, self.transition_reward = self._build_transition_matrices()\n",
        "\n",
        "    def _generate_obstacles(self):\n",
        "        \"\"\"Generate random obstacles covering 15% of the grid (excluding start and goal)\"\"\"\n",
        "        num_obstacles = int(0.15 * self.grid_size**2)\n",
        "        start = (0, 0)\n",
        "        goal = (self.grid_size-1, self.grid_size-1)\n",
        "        available = []\n",
        "\n",
        "        for i in range(self.grid_size):\n",
        "            for j in range(self.grid_size):\n",
        "                if (i, j) != start and (i, j) != goal:\n",
        "                    available.append((i, j))\n",
        "\n",
        "        for idx in np.random.choice(len(available), size=num_obstacles, replace=False):\n",
        "            i, j = available[idx]\n",
        "            self.grid_obstacles[i, j] = True\n",
        "\n",
        "    def _get_next_state(self, state, action):\n",
        "        \"\"\"Returns (next_state, reward) for a given state-action pair\"\"\"\n",
        "        i, j = state // self.grid_size, state % self.grid_size\n",
        "\n",
        "        # Calculate intended movement\n",
        "        if action == 0:  # UP\n",
        "            ni, nj = max(i-1, 0), j\n",
        "        elif action == 1:  # DOWN\n",
        "            ni, nj = min(i+1, self.grid_size-1), j\n",
        "        elif action == 2:  # LEFT\n",
        "            ni, nj = i, max(j-1, 0)\n",
        "        else:  # RIGHT\n",
        "            ni, nj = i, min(j+1, self.grid_size-1)\n",
        "\n",
        "        # Check for obstacle collision\n",
        "        if self.grid_obstacles[ni, nj]:\n",
        "            return state, -0.1  # Penalize hitting obstacle\n",
        "        else:\n",
        "            next_state = ni * self.grid_size + nj\n",
        "            return next_state, self.grid_rewards[ni, nj]\n",
        "\n",
        "    def _build_transition_matrices(self):\n",
        "        \"\"\"Build transition probability and reward matrices\"\"\"\n",
        "        transition_prob = np.zeros((self.num_states, self.num_actions, self.num_states))\n",
        "        transition_reward = np.zeros((self.num_states, self.num_actions, self.num_states))\n",
        "\n",
        "        for s in range(self.num_states):\n",
        "            for a in range(self.num_actions):\n",
        "                next_state, reward = self._get_next_state(s, a)\n",
        "                transition_prob[s, a, next_state] = 1.0\n",
        "                transition_reward[s, a, next_state] = reward\n",
        "\n",
        "        return transition_prob, transition_reward\n",
        "\n",
        "    def iterative_policy_eval(self, policy, theta=1e-4, max_iter=1000):\n",
        "        V = np.zeros(self.num_states)\n",
        "        iterations = 0\n",
        "        for _ in range(max_iter):\n",
        "            iterations += 1\n",
        "            delta = 0\n",
        "            for s in range(self.num_states):\n",
        "                if s == self.num_states - 1:  # Skip terminal state\n",
        "                    continue\n",
        "                v_old = V[s]\n",
        "                a = policy[s]\n",
        "                V[s] = sum(\n",
        "                    self.transition_prob[s, a, s1] *\n",
        "                    (self.transition_reward[s, a, s1] + self.gamma * V[s1])\n",
        "                    for s1 in range(self.num_states)\n",
        "                )\n",
        "                delta = max(delta, abs(v_old - V[s]))\n",
        "            if delta < theta:\n",
        "                break\n",
        "        return V, iterations\n",
        "\n",
        "    def policy_itr(self, theta=1e-4, max_iter=1000):\n",
        "        policy = np.random.randint(0, self.num_actions, self.num_states)\n",
        "        policy_stable = False\n",
        "        total_iterations = 0\n",
        "        outer_iterations = 0\n",
        "\n",
        "        while not policy_stable and outer_iterations < max_iter:\n",
        "            outer_iterations += 1\n",
        "            # Policy Evaluation\n",
        "            V, eval_iterations = self.iterative_policy_eval(policy, theta, max_iter)\n",
        "            total_iterations += eval_iterations\n",
        "\n",
        "            # Policy Improvement\n",
        "            policy_stable = True\n",
        "            for s in range(self.num_states):\n",
        "                if s == self.num_states - 1:  # Skip terminal state\n",
        "                    continue\n",
        "                old_action = policy[s]\n",
        "                q_values = np.zeros(self.num_actions)\n",
        "                for a in range(self.num_actions):\n",
        "                    q_values[a] = sum(\n",
        "                        self.transition_prob[s, a, s1] *\n",
        "                        (self.transition_reward[s, a, s1] + self.gamma * V[s1])\n",
        "                        for s1 in range(self.num_states)\n",
        "                    )\n",
        "                policy[s] = np.argmax(q_values)\n",
        "                if old_action != policy[s]:\n",
        "                    policy_stable = False\n",
        "        return policy, V, total_iterations\n",
        "\n",
        "    def value_itr(self, theta=1e-4, max_iter=1000):\n",
        "        V = np.zeros(self.num_states)\n",
        "        iterations = 0\n",
        "        for _ in range(max_iter):\n",
        "            iterations += 1\n",
        "            delta = 0\n",
        "            for s in range(self.num_states):\n",
        "                if s == self.num_states - 1:  # Skip terminal state\n",
        "                    continue\n",
        "                v_old = V[s]\n",
        "                max_q = -np.inf\n",
        "                for a in range(self.num_actions):\n",
        "                    q = sum(\n",
        "                        self.transition_prob[s, a, s1] *\n",
        "                        (self.transition_reward[s, a, s1] + self.gamma * V[s1])\n",
        "                        for s1 in range(self.num_states)\n",
        "                    )\n",
        "                    max_q = max(max_q, q)\n",
        "                V[s] = max_q\n",
        "                delta = max(delta, abs(v_old - V[s]))\n",
        "            if delta < theta:\n",
        "                break\n",
        "\n",
        "        # Derive policy\n",
        "        policy = np.zeros(self.num_states, dtype=int)\n",
        "        for s in range(self.num_states):\n",
        "            q_values = np.zeros(self.num_actions)\n",
        "            for a in range(self.num_actions):\n",
        "                q_values[a] = sum(\n",
        "                    self.transition_prob[s, a, s1] *\n",
        "                    (self.transition_reward[s, a, s1] + self.gamma * V[s1])\n",
        "                    for s1 in range(self.num_states)\n",
        "                )\n",
        "            policy[s] = np.argmax(q_values)\n",
        "        return policy, V, iterations\n",
        "\n",
        "    def visualize_grid(self):\n",
        "        \"\"\"Create a visual representation of the grid with obstacles and goal\"\"\"\n",
        "        grid = np.full((self.grid_size, self.grid_size), '.', dtype='U4')\n",
        "        grid[self.grid_obstacles] = 'X'\n",
        "        grid[self.goal] = 'G'\n",
        "        grid[0, 0] = 'S'  # Start state\n",
        "        return grid\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    np.random.seed(42)  # For reproducibility\n",
        "    grid_size = 16\n",
        "    env = GridWorld(grid_size)\n",
        "\n",
        "    # Show grid layout\n",
        "    print(\"Grid Layout:\")\n",
        "    visual_grid = env.visualize_grid()\n",
        "    for row in visual_grid:\n",
        "        print(' '.join(row))\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Run Policy Iteration\n",
        "    pi_policy, pi_values, pi_iterations = env.policy_itr()\n",
        "    print(\"Policy Iteration Results:\")\n",
        "    print(f\"Total iterations: {pi_iterations}\")\n",
        "    print(\"Optimal Policy (0=UP, 1=DOWN, 2=LEFT, 3=RIGHT):\")\n",
        "    print(pi_policy.reshape(grid_size, grid_size))\n",
        "    print(\"Value Function:\")\n",
        "    print(pi_values.reshape(grid_size, grid_size).round(2))\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Run Value Iteration\n",
        "    vi_policy, vi_values, vi_iterations = env.value_itr()\n",
        "    print(\"Value Iteration Results:\")\n",
        "    print(f\"Total iterations: {vi_iterations}\")\n",
        "    print(\"Optimal Policy (0=UP, 1=DOWN, 2=LEFT, 3=RIGHT):\")\n",
        "    print(vi_policy.reshape(grid_size, grid_size))\n",
        "    print(\"Value Function:\")\n",
        "    print(vi_values.reshape(grid_size, grid_size).round(2))\n",
        "    print(\"\\n\")\n",
        "\n",
        "    # Compare methods\n",
        "    if pi_iterations < vi_iterations:\n",
        "        print(f\"Policy Iteration was faster ({pi_iterations} iterations vs {vi_iterations})\")\n",
        "    elif pi_iterations > vi_iterations:\n",
        "        print(f\"Value Iteration was faster ({vi_iterations} iterations vs {pi_iterations})\")\n",
        "    else:\n",
        "        print(f\"Both methods took the same number of iterations ({pi_iterations})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yid0ct1op5UD",
        "outputId": "56aa93ea-3433-41e2-efa6-6c8ee8a3f0bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grid Layout:\n",
            "S . . . . . . X . . X X . . . .\n",
            "X . . X X . . . . X X . . . . X\n",
            ". . X . . . . . . . . . . . . .\n",
            ". . . . . . . . . . . . . . . .\n",
            ". . . . . X . . . . . . . . . .\n",
            "X . . . . . . X . . . . . . . .\n",
            ". X X . . . . . . . . . . . . .\n",
            ". . . . . . X . . X . . . X . .\n",
            ". . . . . . . . . X . . . . . .\n",
            ". . . . . X . . . X . . . . . .\n",
            ". . . . . X . . X . . . . . . .\n",
            ". . . . . X . . . X X . . . . X\n",
            ". . . . . . . X . . X . . . X .\n",
            ". . X . . . . . . X . . . X X .\n",
            ". . . . . X . . . . X . . . . .\n",
            ". . . X . . . . . . . . X . . G\n",
            "\n",
            "\n",
            "Policy Iteration Results:\n",
            "Total iterations: 543\n",
            "Optimal Policy (0=UP, 1=DOWN, 2=LEFT, 3=RIGHT):\n",
            "[[3 1 3 3 3 1 1 1 1 2 2 1 1 1 1 2]\n",
            " [1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1]\n",
            " [3 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1]\n",
            " [3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            " [1 0 3 3 3 3 3 3 3 3 1 1 1 2 1 1]\n",
            " [1 1 1 0 0 0 0 0 0 3 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 3 1 1 0 3 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 3 1 1 2 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 3 1 1 3 3 3 1 1 1 1 2]\n",
            " [1 1 1 1 1 1 1 3 1 0 3 1 1 1 2 1]\n",
            " [1 1 3 1 1 1 1 1 1 2 1 1 1 2 3 1]\n",
            " [1 1 1 1 1 3 1 1 1 3 3 1 1 1 1 1]\n",
            " [3 3 3 3 1 1 1 1 1 1 3 3 3 1 1 1]\n",
            " [0 0 0 3 3 3 3 3 3 3 3 0 3 3 3 2]]\n",
            "Value Function:\n",
            "[[0.05 0.05 0.06 0.06 0.07 0.08 0.09 0.1  0.11 0.1  0.09 0.15 0.17 0.15\n",
            "  0.14 0.12]\n",
            " [0.05 0.06 0.05 0.07 0.08 0.09 0.1  0.11 0.12 0.14 0.15 0.17 0.19 0.17\n",
            "  0.15 0.14]\n",
            " [0.06 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.14 0.15 0.17 0.19 0.21 0.19\n",
            "  0.17 0.15]\n",
            " [0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.14 0.15 0.17 0.19 0.21 0.23 0.21\n",
            "  0.19 0.17]\n",
            " [0.07 0.08 0.09 0.1  0.11 0.12 0.14 0.15 0.17 0.19 0.21 0.23 0.25 0.23\n",
            "  0.21 0.19]\n",
            " [0.08 0.09 0.1  0.11 0.12 0.14 0.15 0.17 0.19 0.21 0.23 0.25 0.28 0.25\n",
            "  0.23 0.21]\n",
            " [0.07 0.08 0.11 0.12 0.14 0.15 0.17 0.19 0.21 0.23 0.25 0.28 0.31 0.28\n",
            "  0.25 0.23]\n",
            " [0.08 0.09 0.1  0.11 0.12 0.14 0.15 0.17 0.19 0.25 0.28 0.31 0.35 0.31\n",
            "  0.28 0.25]\n",
            " [0.09 0.1  0.11 0.12 0.14 0.15 0.17 0.19 0.17 0.28 0.31 0.35 0.39 0.35\n",
            "  0.31 0.28]\n",
            " [0.1  0.11 0.12 0.14 0.15 0.17 0.19 0.21 0.19 0.31 0.35 0.39 0.43 0.39\n",
            "  0.35 0.31]\n",
            " [0.11 0.12 0.14 0.15 0.17 0.19 0.21 0.23 0.31 0.35 0.39 0.43 0.48 0.43\n",
            "  0.39 0.35]\n",
            " [0.12 0.14 0.15 0.17 0.19 0.21 0.23 0.25 0.28 0.31 0.43 0.48 0.53 0.48\n",
            "  0.43 0.73]\n",
            " [0.14 0.15 0.17 0.19 0.21 0.23 0.25 0.28 0.31 0.28 0.48 0.53 0.59 0.53\n",
            "  0.73 0.81]\n",
            " [0.15 0.17 0.19 0.21 0.23 0.25 0.28 0.31 0.35 0.48 0.53 0.59 0.66 0.73\n",
            "  0.81 0.9 ]\n",
            " [0.17 0.19 0.21 0.23 0.25 0.28 0.31 0.35 0.39 0.43 0.59 0.66 0.73 0.81\n",
            "  0.9  1.  ]\n",
            " [0.15 0.17 0.19 0.25 0.28 0.31 0.35 0.39 0.43 0.48 0.53 0.59 0.81 0.9\n",
            "  1.   0.  ]]\n",
            "\n",
            "\n",
            "Value Iteration Results:\n",
            "Total iterations: 31\n",
            "Optimal Policy (0=UP, 1=DOWN, 2=LEFT, 3=RIGHT):\n",
            "[[3 1 3 3 3 1 1 1 1 2 2 1 1 1 1 2]\n",
            " [1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1]\n",
            " [3 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1]\n",
            " [3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            " [1 0 3 3 3 3 3 3 3 3 1 1 1 2 1 1]\n",
            " [1 1 1 0 0 0 0 0 0 3 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 3 1 1 0 3 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 3 1 1 2 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 3 1 1 3 3 3 1 1 1 1 2]\n",
            " [1 1 1 1 1 1 1 3 1 0 3 1 1 1 2 1]\n",
            " [1 1 3 1 1 1 1 1 1 2 1 1 1 2 3 1]\n",
            " [1 1 1 1 1 3 1 1 1 3 3 1 1 1 1 1]\n",
            " [3 3 3 3 1 1 1 1 1 1 3 3 3 1 1 1]\n",
            " [0 0 0 3 3 3 3 3 3 3 3 0 3 3 3 1]]\n",
            "Value Function:\n",
            "[[0.05 0.05 0.06 0.06 0.07 0.08 0.09 0.1  0.11 0.1  0.09 0.15 0.17 0.15\n",
            "  0.14 0.12]\n",
            " [0.05 0.06 0.05 0.07 0.08 0.09 0.1  0.11 0.12 0.14 0.15 0.17 0.19 0.17\n",
            "  0.15 0.14]\n",
            " [0.06 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.14 0.15 0.17 0.19 0.21 0.19\n",
            "  0.17 0.15]\n",
            " [0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.14 0.15 0.17 0.19 0.21 0.23 0.21\n",
            "  0.19 0.17]\n",
            " [0.07 0.08 0.09 0.1  0.11 0.12 0.14 0.15 0.17 0.19 0.21 0.23 0.25 0.23\n",
            "  0.21 0.19]\n",
            " [0.08 0.09 0.1  0.11 0.12 0.14 0.15 0.17 0.19 0.21 0.23 0.25 0.28 0.25\n",
            "  0.23 0.21]\n",
            " [0.07 0.08 0.11 0.12 0.14 0.15 0.17 0.19 0.21 0.23 0.25 0.28 0.31 0.28\n",
            "  0.25 0.23]\n",
            " [0.08 0.09 0.1  0.11 0.12 0.14 0.15 0.17 0.19 0.25 0.28 0.31 0.35 0.31\n",
            "  0.28 0.25]\n",
            " [0.09 0.1  0.11 0.12 0.14 0.15 0.17 0.19 0.17 0.28 0.31 0.35 0.39 0.35\n",
            "  0.31 0.28]\n",
            " [0.1  0.11 0.12 0.14 0.15 0.17 0.19 0.21 0.19 0.31 0.35 0.39 0.43 0.39\n",
            "  0.35 0.31]\n",
            " [0.11 0.12 0.14 0.15 0.17 0.19 0.21 0.23 0.31 0.35 0.39 0.43 0.48 0.43\n",
            "  0.39 0.35]\n",
            " [0.12 0.14 0.15 0.17 0.19 0.21 0.23 0.25 0.28 0.31 0.43 0.48 0.53 0.48\n",
            "  0.43 0.73]\n",
            " [0.14 0.15 0.17 0.19 0.21 0.23 0.25 0.28 0.31 0.28 0.48 0.53 0.59 0.53\n",
            "  0.73 0.81]\n",
            " [0.15 0.17 0.19 0.21 0.23 0.25 0.28 0.31 0.35 0.48 0.53 0.59 0.66 0.73\n",
            "  0.81 0.9 ]\n",
            " [0.17 0.19 0.21 0.23 0.25 0.28 0.31 0.35 0.39 0.43 0.59 0.66 0.73 0.81\n",
            "  0.9  1.  ]\n",
            " [0.15 0.17 0.19 0.25 0.28 0.31 0.35 0.39 0.43 0.48 0.53 0.59 0.81 0.9\n",
            "  1.   0.  ]]\n",
            "\n",
            "\n",
            "Value Iteration was faster (31 iterations vs 543)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Setup**: A 16x16 grid with 15% random obstacles (marked 'X'), a start at (0,0) ('S'), goal at (15,15) ('G'), and a -0.1 penalty for hitting obstacles.\n",
        "- **Algorithms**: Enhanced to handle obstacles via transition and reward matrices.\n",
        "- **Results**:\n",
        "  - **Policy Iteration**: Takes 543 iterations, optimal policy navigates around obstacles toward the goal, value function reflects penalties (e.g., dips near obstacles) and peaks at 1.0.\n",
        "  - **Value Iteration**: Converges in 31 iterations, yielding an almost identical policy and value function.\n",
        "  - Grid visualization shows obstacle placement, with policies adapting to avoid 'X' cells.\n",
        "- **Comparison**: Value Iteration remains faster (31 vs. 543 iterations), despite the added complexity of obstacles.\n"
      ],
      "metadata": {
        "id": "HIIh_NO58H8a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Key Insights\n",
        "- **Environment Complexity**: The experiment progresses from a simple 3x3 grid to a 16x16 grid with obstacles, testing algorithm robustness.\n",
        "- **Algorithm Performance**:\n",
        "  - Iterative Policy Evaluation is limited to fixed policies and serves as a baseline.\n",
        "  - Policy Iteration, while effective, requires more iterations due to repeated policy evaluation.\n",
        "  - Value Iteration consistently converges faster by directly optimizing the value function.\n",
        "- **Convergence**: Both Policy and Value Iteration find optimal policies, but Value Iteration’s single-pass approach outperforms in larger, complex grids.\n",
        "- **Practicality**: Obstacles introduce realistic navigation challenges, and the penalty (-0.1) shapes policies to avoid them, validated by value function dips."
      ],
      "metadata": {
        "id": "OAtMk-nN8Y2q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###*The experiment demonstrates that Value Iteration is generally more efficient than Policy Iteration in terms of iterations, especially in larger grids or with obstacles, making it preferable for computational efficiency. All algorithms successfully solve the GridWorld problem, with results aligning with reinforcement learning theory—higher values and directed policies toward the goal.*"
      ],
      "metadata": {
        "id": "IqJEe-av7M_q"
      }
    }
  ]
}